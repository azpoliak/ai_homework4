# ai_homework4
Implementation of ML algorithms for AI Homework4

Adam Poliak, Sayge Schell
Dec 4th, 2015

To run, do the following:

Decision Trees:

python train_and_test.py -d=congress -p=75 -m=0-0-0 -test

Naive Bayes:
python train_and_test.py -d=congress -p=75 -m=1 -test

Neural Networks:
python train_and_test.py -d=congress -p=75 -m=0-0-0 -test


***Notes:
-d refers to the data set
-p refers to the percent, if -d is monk, then instead of -p=(int), do 1, 2, or 3 to determine with monk data set
-m refers to machine learning method.
    - If m=0, the 2nd number indicates whether or not pruning is set (0 is not set, 1 is set), 3rd number indicates whether or not information gain ration is set (0 for not set, 1 is set)

**Class Descriptions**
decision_tree.py - Contains structure for decision tree
naive_bayes.py - Contains structure for naive bayes
neural_network.py - Contains structure for neural network
train_and_test.py - Driver program to use structures

Please note, for precision and recall, '0' was selected for the value for the 'positive' one. All other values were set
to be negatives.

*Decision Trees* -
Key: IGR = Information Gain ratio, IG = Information Gain, NP = No pruning, P = Pruning

Congress- 
            IGR-NP          IG - NP             IGR -P              IGR - NP
Accuracy:   0.935779815614  0.926605504587
Precision:  0.931818181818  0.911111111111
Recall:     0.911111111111  0.911111111111

Monk 1 -

Accuracy:   0.916666666667  0.796296296296
Precision:  1.0             1.0
Recall:     0.888888888889  0.703703703704

Monk2 -

Accuracy:  0.530092592593   0.511574074074
Precision: 0.741228070175   0.757281553398
Recall:    0.58275862069    0.537931034483

Monk3 -

Accuracy:   0.91666666667   0.925925925926
Precision:  0.941176470588  0.96
Recall:     0.941176470588  0.941176470588

Iris-

Accuracy:   0.526315789474  0.710526315789
Precision:  0.75            1.0
Recall:     0.75            0.916666666667

In general, using the information gain ratio will raise the accuracy, recall, and precision of the predictions for the
test data. That is, generally, it is better to use the information gain ratio to determine the split in the tree rather
than just the pure information gain. However, in the case of Iris and Monk3, accuracy, precision, and recall were all
lowered when using the information gain ratio, suggesting that using this is not necessarily better in all cases.

*Naive Bayes*-

            Congress            Monk1           Monk2           Monk3           Iris
Accuracy:   .889908256881       .576388888889   .643259259259   .731481481481   .947368421053
Precision:  .971428571429       .648648648649   .674603174603   .6375           1.0
Recall:     .755555555556       .333333333333   .879310344828   1.0             1.0

*Neural Network* - 
Key: DW = default weight, AW = alternate weight, M = with momentum, NM = without momentum
            DW-NM           AW-NM           DW-M            AW-M
Congress-

Accuracy:
Precision:
Recall:

Monk1 -

Accuracy:
Precision:
Recall:

Monk2 -

Accuracy:
Precision:
Recall:

Monk3 -

Accuracy:
Precision:
Recall:

Iris-

Accuracy:
Precision:
Recall:

Discuss how the performances of the algorithms compare on each data set.

*Best Algorithms for each data set*
Congress:
Monk1:
Monk2:
Monk3:
Iris:

Which metrics do you use to determine this and how does the nature of the data set affect your decision?

Work Breakdown: 
Naive Bayes + Neural Networks : Adam
Decision Trees + Data: Sayge

Bugs: TODO
